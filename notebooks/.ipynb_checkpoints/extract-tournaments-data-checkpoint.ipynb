{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and Clean Data from all_tournaments.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero creamos un Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import  (StructType, StructField, DateType, BooleanType, DoubleType, IntegerType, StringType, TimestampType)\n",
    "from pyspark.sql.functions import col, udf\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"tenis-matches\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fedsa\\Documents\\ORT\\Semestre 9\\Big Data\\Obligatorio\\dataset\\raw\\all_tournaments.csv\n"
     ]
    }
   ],
   "source": [
    "raw_file_path = os.path.join(os.path.abspath(os.path.pardir), \"dataset\", \"raw\", \"all_tournaments.csv\")\n",
    "tor = spark.read.csv(\"file:///\" + raw_file_path, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 36488 tournaments.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} tournaments.\".format(tor.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminaremos algunas columnas del dataset que no nos sirven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- tournament: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- end_date: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- court_surface: string (nullable = true)\n",
      " |-- prize_money: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- masters: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tor.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminaremos las siguientes columnas que no serán útiles para nuestro análisis:\n",
    "- prize_money\n",
    "- currency\n",
    "- masters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tor = tor.drop(\"prize_money\").drop(\"currency\").drop(\"masters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- tournament: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- end_date: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- court_surface: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tor.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No eliminaremos torneos con valores nulos, ya que puede generar inconsistencias con los otros datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_file_path = os.path.join(os.path.abspath(os.path.pardir), \"dataset\", \"processed\", \"all_tournaments.csv\")\n",
    "tor.write.format(\"csv\").option(\"header\", True).mode('overwrite').save(\"file:///\" + processed_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creacion de archivo para ejecutar extraccion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "get_processed_data_script_file = os.path.join(os.path.pardir, \"process_all_tournaments.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ..\\process_all_tournaments.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $get_processed_data_script_file\n",
    "\n",
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import  (StructType, StructField, DateType, BooleanType, DoubleType, IntegerType, StringType, TimestampType)\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "def extract_data():\n",
    "    spark = SparkSession.builder.master(\"local[1]\").appName(\"tenis-matches\").getOrCreate()\n",
    "    \n",
    "    raw_file_path = os.path.join(os.path.abspath(os.path.pardir), \"dataset\", \"raw\", \"all_tournaments.csv\")\n",
    "    tor = spark.read.csv(\"file:///\" + raw_file_path, header = True)\n",
    "    \n",
    "    tor = tor.drop(\"prize_money\").drop(\"currency\").drop(\"masters\")\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    df = extract_data()\n",
    "    processed_file_path = os.path.join(os.path.abspath(os.path.pardir), \"dataset\", \"processed\", \"all_tournaments.csv\")\n",
    "    df.write.format(\"csv\").option(\"header\", True).mode('overwrite').save(\"file:///\" + processed_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2020-11-09 22:44:24,919 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2020-11-09 22:44:24,920 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "2020-11-09 22:44:24,920 WARN util.Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "Traceback (most recent call last):\n",
      "  File \"..\\process_all_tournaments.py\", line 22, in <module>\n",
      "    tor.write.format(\"csv\").option(\"header\", True).mode('overwrite').save(\"file:///\" + processed_file_path)\n",
      "NameError: name 'tor' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: The process with PID 19964 (child process of PID 21496) has been terminated.\n",
      "SUCCESS: The process with PID 21496 (child process of PID 10092) has been terminated.\n",
      "SUCCESS: The process with PID 10092 (child process of PID 20204) has been terminated.\n"
     ]
    }
   ],
   "source": [
    "!python $get_processed_data_script_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
