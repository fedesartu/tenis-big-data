{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and Clean Data from all_tournaments.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero creamos un Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import  (StructType, StructField, DateType, BooleanType, DoubleType, IntegerType, StringType, TimestampType)\n",
    "from pyspark.sql.functions import col, udf\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"tenis-matches\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file_path = os.path.join(os.path.abspath(os.path.pardir), \"dataset\", \"raw\", \"all_tournaments.csv\")\n",
    "tor = spark.read.csv(\"file:///\" + raw_file_path, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 36488 tournaments.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} tournaments.\".format(tor.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminaremos algunas columnas del dataset que no nos sirven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- tournament: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- end_date: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- court_surface: string (nullable = true)\n",
      " |-- prize_money: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- masters: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tor.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminaremos las siguientes columnas que no serán útiles para nuestro análisis:\n",
    "- prize_money\n",
    "- currency\n",
    "- masters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tor = tor.drop(\"prize_money\").drop(\"currency\").drop(\"masters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- tournament: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- end_date: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- court_surface: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tor.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No eliminaremos torneos con valores nulos, ya que puede generar inconsistencias con los otros datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminaremos las \" de todos los nombres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+\n",
      "|tournament                                      |\n",
      "+------------------------------------------------+\n",
      "||tf-junior-circuit---group-4_itf_juniors1       |\n",
      "||tf-junior-circuit---group-4_itf_juniors0       |\n",
      "||tf-junior-circuit---colombo-week-2_itf_juniors1|\n",
      "||tf-junior-circuit---colombo-week-2_itf_juniors0|\n",
      "|zurich                                          |\n",
      "|zurich                                          |\n",
      "|ztk-junior-open_itf_juniors1                    |\n",
      "|ztk-junior-open_itf_juniors1                    |\n",
      "|ztk-junior-open_itf_juniors1                    |\n",
      "|ztk-junior-open_itf_juniors1                    |\n",
      "+------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import asc, desc\n",
    "\n",
    "tor.select(\"tournament\").orderBy(desc(\"tournament\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeCharacter(df, column, char):\n",
    "    removeFn = udf(lambda x: x.replace(char, ''), StringType())\n",
    "    return df.withColumn(column, removeFn(col(column)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+\n",
      "|tournament                                      |\n",
      "+------------------------------------------------+\n",
      "||tf-junior-circuit---group-4_itf_juniors1       |\n",
      "||tf-junior-circuit---group-4_itf_juniors0       |\n",
      "||tf-junior-circuit---colombo-week-2_itf_juniors1|\n",
      "||tf-junior-circuit---colombo-week-2_itf_juniors0|\n",
      "|zurich                                          |\n",
      "|zurich                                          |\n",
      "|ztk-junior-open_itf_juniors1                    |\n",
      "|ztk-junior-open_itf_juniors1                    |\n",
      "|ztk-junior-open_itf_juniors1                    |\n",
      "|ztk-junior-open_itf_juniors1                    |\n",
      "+------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tor = removeCharacter(tor, \"tournament\", '\"')\n",
    "tor.select(\"tournament\").orderBy(desc(\"tournament\")).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen algunos torneos sin datos de finalizacion. Para estos, le pondremos como fecha de finalizacion la misma que de comienzo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero cambiamos los tipos a Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego cambiamos los start_date null a 1/1/año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- tournament: string (nullable = true)\n",
      " |-- start_date: date (nullable = true)\n",
      " |-- end_date: date (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- court_surface: string (nullable = true)\n",
      "\n",
      "+----+----------+----------+--------+--------+-------------+\n",
      "|year|tournament|start_date|end_date|location|court_surface|\n",
      "+----+----------+----------+--------+--------+-------------+\n",
      "|1974| las-vegas|      null|    null|     USA|         Clay|\n",
      "|1974|     tokyo|      null|    null|   Japan|         Hard|\n",
      "+----+----------+----------+--------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp, to_date, when\n",
    "\n",
    "tor = new_df.withColumn('start_date', to_date(unix_timestamp(col('start_date'), 'yyyy-mm-dd').cast(\"timestamp\"))).withColumn('end_date', to_date(unix_timestamp(col('end_date'), 'yyyy-mm-dd').cast(\"timestamp\")))\n",
    "tor.printSchema()\n",
    "tor.select(\"*\").where(tor[\"start_date\"].isNull()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+--------+--------+-------------+\n",
      "|year|tournament|start_date|end_date|location|court_surface|\n",
      "+----+----------+----------+--------+--------+-------------+\n",
      "+----+----------+----------+--------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp, to_date, when, lit\n",
    "import datetime\n",
    "\n",
    "tor = tor.withColumn(\"start_date\", when(col(\"start_date\").isNull(), lit(datetime.datetime(1974, 1, 1))).otherwise(col(\"start_date\")))\n",
    "tor = tor.withColumn('start_date', to_date(unix_timestamp(col('start_date'), 'yyyy-mm-dd').cast(\"timestamp\"))).withColumn('end_date', to_date(unix_timestamp(col('end_date'), 'yyyy-mm-dd').cast(\"timestamp\")))\n",
    "\n",
    "tor.select(\"*\").where(tor[\"start_date\"].isNull()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "tor = tor.withColumn(\"end_date\", coalesce(tor[\"end_date\"], tor[\"start_date\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- tournament: string (nullable = true)\n",
      " |-- start_date: date (nullable = true)\n",
      " |-- end_date: date (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- court_surface: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tor.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_file_path = os.path.join(os.path.abspath(os.path.pardir), \"dataset\", \"processed\", \"all_tournaments.csv\")\n",
    "tor.write.format(\"csv\").option(\"header\", True).mode('overwrite').save(\"file:///\" + processed_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creacion de archivo para ejecutar extraccion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "get_processed_data_script_file = os.path.join(os.path.pardir, \"process_all_tournaments.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ..\\process_all_tournaments.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $get_processed_data_script_file\n",
    "\n",
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import  (StructType, StructField, DateType, BooleanType, DoubleType, IntegerType, StringType, TimestampType)\n",
    "from pyspark.sql.functions import col, unix_timestamp, to_date, when, lit, udf, coalesce\n",
    "import datetime\n",
    "\n",
    "def extract_data():\n",
    "    spark = SparkSession.builder.master(\"local[1]\").appName(\"tenis-matches\").getOrCreate()\n",
    "    \n",
    "    raw_file_path = os.path.join(os.path.abspath(os.path.pardir), \"dataset\", \"raw\", \"all_tournaments.csv\")\n",
    "    tor = spark.read.csv(\"file:///\" + raw_file_path, header = True)\n",
    "    \n",
    "    tor = tor.drop(\"prize_money\").drop(\"currency\").drop(\"masters\")\n",
    "    \n",
    "    tor = removeCharacter(tor, \"tournament\", '\"')\n",
    "    \n",
    "    tor = tor.withColumn('start_date', to_date(unix_timestamp(col('start_date'), 'yyyy-mm-dd').cast(\"timestamp\"))).withColumn('end_date', to_date(unix_timestamp(col('end_date'), 'yyyy-mm-dd').cast(\"timestamp\")))\n",
    "    tor = tor.withColumn(\"start_date\", when(col(\"start_date\").isNull(), lit(datetime.datetime(1974, 1, 1))).otherwise(col(\"start_date\")))\n",
    "    tor = tor.withColumn('start_date', to_date(unix_timestamp(col('start_date'), 'yyyy-mm-dd').cast(\"timestamp\"))).withColumn('end_date', to_date(unix_timestamp(col('end_date'), 'yyyy-mm-dd').cast(\"timestamp\")))\n",
    "\n",
    "    tor = tor.withColumn(\"end_date\", coalesce(tor[\"end_date\"], tor[\"start_date\"]))\n",
    "    \n",
    "    return tor\n",
    "\n",
    "def removeCharacter(df, column, char):\n",
    "    removeFn = udf(lambda x: x.replace(char, ''), StringType())\n",
    "    return df.withColumn(column, removeFn(col(column)))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    df = extract_data()\n",
    "    processed_file_path = os.path.join(os.path.abspath(os.path.pardir), \"dataset\", \"processed\", \"all_tournaments.csv\")\n",
    "    df.write.format(\"csv\").option(\"header\", True).mode('overwrite').save(\"file:///\" + processed_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2020-11-16 23:11:24,858 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2020-11-16 23:11:24,859 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\n",
      "                                                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: The process with PID 1392 (child process of PID 5880) has been terminated.\n",
      "SUCCESS: The process with PID 5880 (child process of PID 14564) has been terminated.\n",
      "SUCCESS: The process with PID 14564 (child process of PID 24176) has been terminated.\n"
     ]
    }
   ],
   "source": [
    "!python $get_processed_data_script_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
